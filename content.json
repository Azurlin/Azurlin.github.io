{"pages":[],"posts":[{"title":"Hello World 你好","text":"测试","link":"/2020/06/25/hello-world/"},{"title":"ZAB协议","text":"一、概述 ZAB（Zookeeper Atomic Broadcast）协议是一套专门为zookeeper设计的进行原子广播和崩溃恢复的协议。 ZAB是基于2PC算法来进行设计的并且利用过半性+PAXOS算法进行了改进。 二、原子广播 作用：保证节点之间数据的一致性，在zookeeper中，访问任意一个节点获取到的数据都是相同的。 原子广播基于2PC算法设计，利用过半性来改进 2PC（Two Phase Commit）二阶段提交 要么请求提交要么请求中止，核心思想是一票否决。 请求阶段：协调者接收到请求不能立即决定是否立即执行，而是需要将请求发送给每一个参与者，等待参与者的反馈信息。 提交阶段：如果协调者收到所有的参与者返回的yes，那么协调者就会命令每一个参与者执行这个请求。 中止阶段：如果协调者没有收到所有参与者返回的yes，那么协调者就会命令所有的参与者放弃这个请求。 2PC算法设计和实现过程相对简单，但是容易受外界环境影响，对网络环境的容错性相对较差。 原子广播过程： leader接收到请求之后会先将这个请求记录到本地的日志文件log.xxxx中。log.xxxx的存储位置由dataDir属性来决定。 如果记录失败则leader会认为这个请求不能执行，直接报错。 如果记录成功则leader会把请求放到队列中发送给每一个follower。 follower在收到队列之后，也会将请求从队列中依次取出然后一一记录到本地的日志文件log.xxxx中。 如果follower认为这个请求可以执行那么会返回给leader一个yes 如果follower认为这个请求不可以执行会返回no 判断是否执行 如果leader收到半数及以上节点返回yes那么就认为这个请求可以执行，就会要求所有的follower执行这个请求。（提交） 如果没有收到到半数yes则认为这个请求不能执行，就会要求所有的follower删除之前的记录。（回滚） 如果follower记录日志失败，leader却还要求执行这个请求，此时follower会给leader发出请求，leader收到请求之后会把操作放到队列之中发送给follower，follower会再次试图记录日志再次执行；如果再次记录失败，会重复发请求。因此，如果在维护集群时发现follower频繁给leader发请求，那么说明这个follower可能产生问题（无法记录日志–文件被占用（会自己解决），磁盘坏道，磁盘已满）。 三、崩溃恢复 在Zookeeper集群中，当leader丢失的时候，集群不会停止服务而是会选举产生一个新的leader，这个过程称之为崩溃恢复。 作用：避免单点故障，保证集群的可用性。 当某一个节点重新连入集群后，这个节点会先找到自己的最大事务id，然后会给leader发送请求。leader收到请求之后会去比较事务id是否一致，如果不一致leader会将缺失的事务放到队列中返回给该节点要求补齐。 zookeeper会对每一任leader分配一个递增的epochid。当leader上任之后，这个leader会将自己的epochid分发给每一个follower，而follower收到之后会存储到本地文件acceptedEpoch中。集群中事务id是由64位二进制（16位十六进制）数字组成，其中高32位是epochid,低32位是实际的事务id。例如0x200000009表示第二任leader执行的第九个写操作。","link":"/2020/06/25/BigData/ZAB%E5%8D%8F%E8%AE%AE/"},{"title":"Zookeeper集群","text":"一、概述 zookeeper集群启动后，所有节点（服务器）都会进入选举状态，所有节点都会推荐自己成为leader 每一个节点都会将自己的选举信息发送给其他节点，节点之间会进行比较，经过多轮比较之后最终胜出的节点成为leader 二、集群特性 过半性：过半选举、过半服务、过半操作 数据一致性：访问任意一个节点，获取到的数据时相同的。 原子性：一个请求到了Zookeeper上之后，要么所有节点都执行要么都不执行 可靠性：保证集群的高可用。 顺序性：leader按照什么顺序接收。的请求，那么follower就会按照什么顺序执行请求。 实时性：在网络条件较好的情况下，可以实时监控Zookeeper的变化。 三、选举机制 选举信息 a. 节点的最大事务id（最大事务id越大意味着这个节点存储的数据越多） b. 选举编号 - myid(我们在配置过程中给的编号) c. 逻辑时钟值（记录选举轮次） 比较原则 先比较两个节点的最大事务id，谁大谁赢 。 如果事务id一致，则比较myid，谁大谁赢 。 如果一个节点胜过一半及以上的节点的时候，这个节点会被选举为leader - 过半性。 一个Zookeeper集群已经选举出来leader，为了集群的稳定性而言，无论新添的节点的事务id或者myid是多少，都只能成为follower 。 如果leader产生宕机，那么Zookeeper会重新选举出一个新的leader 。 如果集群中出现多个leader，这种现象称之为脑裂 。 Zookeeper中脑裂产生的条件 ： 集群产生分裂(网络故障) 分裂之后还进行了选举 在Zookeeper集群中，如果存活的节点个数不足一半，则剩余的节点不选举也不对外服务 - 过半性 。 Zookeeper中的节点个数一般是奇数个 - Zookeeper集群的节点个数最少是3个。 Zookeeper会对每一任leader分配一个自增的编号，这个编号称之为epochid。一旦Zookeeper发现存在一个以上的leader的时候，那么会将epocid较小的节点的状态切换为follower。 四、zookeeper集群中的角色 leader 负责写数据，一个zookeeper集群只有一个leader节点。 客户端提交请求之后,先发送到leader,leader作为接收者,广播到每个server。 follower 转发事务请求给leader，处理非事务请求。 参与选举和投票。 observer 在集群中，一个节点一旦被设置为observer，那么这个节点的状态就不会再改变。 observer既不参与选举也不参与投票（原子广播）。但是observer会监听选举或者投票结果，然后根据结果产生对应的操作。（observer是没有选举权的follower） 一个集群中，一般会将90%~97%的节点设置为observer，这么做的目的是为了提高选举和投票效率，减少网络影响。 在zookeeper集群中，observe的存活与否并不影响集群的服务。例如假设一个集群中有21个节点，其中有一个leader，6个follower，14个observe，那么即使14个observe全部宕机，集群依然对外服务。但是有4个follower宕机那么集群会停止对外服务，因为此时将不存在过半性–过半性是针对有选举权的节点。 配置观察者 peerType=observer","link":"/2020/06/25/BigData/Zookeeper%E9%9B%86%E7%BE%A4/"},{"title":"HDFS","text":"一、概述 HDFS（Hadoop Distributed File System）是hadoop提供的一套用于进行分布式文件存储的系统。 HDFS是doug仿照Google的GFS（Google File System）来实现的。 云主机启动hadoop 通过本地浏览器访问IP:50070监控hadoop。 二、技术细节基本概述 随着数据量的不断增大，单机存储方式已经不能够适应生产环境，所以我们需要引入分布式存储。 HDFS是一个典型的MS结构：主节点是NameNode，从节点是DataNode。 在HDFS中存储数据时，会对数据进行切分，切成不同的数据块Block。 HDFS会对存入其中的Block进行备份，这个备份称之为副本。HDFS中默认的副本策略是3，既需要复制2次加上原来的副本构成3个副本。 HDFS提供了一套类似于linux的文件系统，即仿照Linux，允许用户产生不同的目录，同时为不同的路径设计权限。根路径是/。 Block 数据块 Block是HDFS中的基本存储单位，即所有的数据都是以block形存储的。 Block默认大小不超过128M(可以通过属性 dfsblocksize调节–hdsf/site.xml 单位是字节默认是)。 如果上传的文件不足一个Block大小，那么这个文件是多大对应的Block就是多大。例如一个文件15M那么对应的Block就是15M。 每一个Block都会分配一个编号BlockID。 HDFS会记录每一个Block上传的时间，但是并不是直接记录时间，而是给时间戳一个编号Generation Stamp。 Block的意义a. 能够存储超大文件 b. 能够迅速的备份 一个DataNode可能会存储很多个Block。 NameNode 主节点 NameNode是HDFS中的主节点。 作用：管理DataNode，存储元数据。 元数据是对数据描述的数据，元数据中不包含具体的数据内容 -账本 元数据主要包含1) 文件的上传路径 2) 上传的用户名以及对应的权限 3) 文件的大小 4) Block的大小 5) 文件和BlockID的映射关系 6) BlockID和DataNode的映射关系 7) 文件的副本数量 NameNode会将元数据维系在磁盘以及内存中1) 在磁盘中是为了崩溃恢复 2) 在内存中是为了读写快 元数据的存储位置有属性hadoop.tmp.dir来决定，如果不配置则默认放在/tmp下，所以需要更改路径。 与元数据相关的文件○ edits：操作文件，用于记录HDFS的写操作。 ○ fsimage：元数据（映像）文件，用于记录HDFS的元数据。 当NameNode收到写操作的时候，先将写操作记录到edits_inprogress文件中，如果记录成功，则修改内存中的元数据，如果修改成功，则给客户端返回一个ack信号表示成功。注意，此时fsimage文件中的元数据并没有发生改变。 当edits_inprogress文件达到指定条件的时候，产生一个新的edits_inprogress用来记录新的写操作，同时原来的edits_inprogress会滚动改名为edits，新生成的edits文件会将其中写操作一一取出重新执行来更新fsimage中的元数据。 edits_inprogress滚动条件○ 空间：edits_inprogress文件达到指定大小(默认是64M，通过属性fs.checkpoint.size来调节，配置在core-site.xml，单位是字节)之后，会滚动生成edits。○ 时间：当距离上一次滚动的间隔时间达到指定大小（默认是1h，通过属性fs.checkpoint.period来调节，配置在core-site.xml，单位是秒）的时候，eidts——inprogress也会滚动。○ 重启：当NameNode重启的时候，自动触发edits_inprogress文件的滚动○ 强制滚动：通过hadoop dfadmin -rollEdits来进行强制滚动。 NameNode通过心跳机制来管理DataNode。DataNode定时给NameNode发送心跳信息。 心跳信息通过RPC方式发送。 心跳间隔时间默认为3s，通过属性dfs.heartbeat.interval来调节，单位是秒。 如果NameNode超过10min没有收到DataNode的心跳，则认为这个DataNode已经lost（丢失），那么就会将这个DataNode上的数据备份到其他节点上保证副本数量。 心跳信息○ clusterid：集群编号。在NameNode被格式化（hadoop namenode -format）的时候，自动计算产生一个clusterid。NameNode会将clusterid发送给每一个DataNode，DataNode收到clusterid之后会存储在本地。之后DataNode和NameNode的每一次通信都需要携带clusterid。NameNode在收到DataNode的信号之后会先校验clusterid是否一致，如果不一致则直接抛弃。如果一致，NameNode才会处理DataNode的请求。DataNode只接受一次clusterid。NameNode每次格式化都会产生一个新的clusterid，如果NameNode被格式化多次，就会发现NameNode联系不上DataNode，此时通过jps命令查看，发现要么缺少NameNode要么缺少DataNode。○ DataNode的节点状态：预服役、服役、预退役。○ Block的存储信息 安全模式（safe mode）1) 当NameNode产生重启的时候，首先会触发edits_inprogress文件的滚动，产生edits文件之后会更新fsimage文件中的元数据。2) 当fsimage文件中的元数据更新完成之后，NameNode会将fsimage中的元数据加载到内存中。3) 元数据加载完成之后，NameNode等待DataNode的心跳。4) 如果NameNode没有收到心跳，则会试图备份这个DataNode上的数据到其他的DataNode上。5) NameNode在收到DataNode的心跳之后会进行校验。校验Block信息（Block大小、数量等）。6) 如果校验失败则NameNode会试图恢复这个数据；恢复完成之后会再次校验。7) 如果所有的DataNode都校验成功，则会自动退出安全模式。 在安全模式中，HDFS不对外提供写服务。 NameNode在重启时会自动进入安全模式。如果在进行操作的时候发现HDFS处在安全模式中，那么需要等待一会儿，等他自动退出安全模式。如果在合理的时间内，HDFS没有退出安全模式，那么说明数据产生了无法挽回的丢失。此时只能强制退出安全模式（hadoop dfsadmin -safemode leave）。 正因为安全模式和副本放置策略的存在，所以在伪分布式中副本数量必须为1 - 副本数量不能超过节点数量。 NameNode在HDFS中处于核心地位，但是在Hadoop1.0，NameNode只能由1个，在Hadoop2.0中通过舍弃SecondaryNameNode可以允许存在2个NameNode，在Hadoop3.0中，NameNode的个数不再限制。 副本放置策略 在HDFS中，默认采用多副本机制，默认副本数量为3，通过dfs.replication属性来进行设置，配置在hdfs-site.xml文件中。 放置策略 a. 第一个副本1) 如果是集群内部上传，则谁上传第一个副本就放在谁身上。 2) 如果是集群外部上传，则谁相对空闲就放到谁身上。 b. 第二个副本1) Hadoop2.7之前：第二个副本是放在和第一个副本不同机架的节点上 2) Hadoop2.7开始：第二个副本是放在和第一个副本相同的机架的节点上 c. 第三个副本1) Hadoop2.7之前：第三个副本是放在和第二个副本相同机架的节点上 2) Hadoop2.7开始：第三个副本是放在和第二个副本不同机架的节点上 d. 更多副本：放置在相对空闲的DataNode上。 机架感知策略 默认情况下，Hadoop的机架感知策略没有被启用，所以HDFS在存放数据的时候都是选择相对空闲的节点来存储。 通过脚本文件来指定机架感知。脚本文件可以是Shell/Python语言编写。 通过一个Map来指定机架，其中将主机的主机名或者IP作为Map的键，然后将机架名作为值。只要值一样，就意味着这些键对应的主机就在一个机架上 - 这个Map称之为逻辑机架。 理论上可以将不同的物理机架上的节点配置在相同的逻辑机架上（只要保持值相同即可），但是这样不利于管理，所以实际过程中，一般是将统一个物理机架上的节点配置在同一个逻辑机架上。 SecondaryNameNode SecondaryNameNode不是NameNode的备份，仅仅是辅助NameNode完成edits文件的滚动和fsimage文件的更新。 如果存在SecondaryNameNode，那么edits文件的滚动和fsimage文件的更新是发生在SecondaryNameNode上。如果不存在，则上述过程发生在NameNode。 到目前为止，HDFS2.0只支持2种结构。a. 1个NameNode+1个SecondaryNameNode+多个DataNode。 b. 2个NameNode（构成备份）+多个DataNode。 考虑到NameNode的核心地位，所以需要考虑NameNode的备份问题，所以一般采用的是第二种结构：2个NameNode(Active状态+Standby备份状态)+多个DataNode。 DataNode 作用：存储数据，数据是以Block形式存在。 DataNode将数据存储在磁盘上（数据多），在磁盘上的存储位置由属性hadoop.tmp.dir来决定。 DataNode会为每一个Block生成一个.meta文件，.meta实际上是对Block的校验（.meta中存储了一个校验值，这个校验值是根据Block大小、生成时间、存储内容等信息进行非对称加密计算出来的一个整数）。DataNode给NameNode发送心跳的时候是包含这个加密值的，如果更改了Block内容，那么.meta文件中的内容也会重新计算。 DataNode通过心跳机制来向NameNode发送信息。 基本命令 命令 解释 hadoop fs -put /home/a.txt /b.txt 将/home/a.txt上传到HDFS的根目录下并且重命名为b.txt hadoop fs -mkdir /log 在HDFS的/下创建子目录log hadoop fs -get /a.txt /home 将HDFS的/a.txt下载到本地的/home目录下 hadoop fs -mv /a.txt /b.txt 重命名 hadoop fs -mv /a.txt /log/a.txt 剪切 hadoop fs -cp /a.txt /c.txt 复制 hadoop fs -chmod 777 /a.txt 修改权限 hadoop fs -ls / 查看/下的子文件和子目录 hadoop fs -lsr / 递归查看 hadoop fs -rm /a.txt 删除文件 hadoop fs -rmdir /test 删除目录，要求目录为空 hadoop fs -rmr /log 递归删除目录 回收站机制 在HDFS中，回收站机制默认是不开启的（即默认值为0），所以一个文件如果被删除，会立即从HDFS上移除，这个操作不可撤销。 回收站机制需要core-site.xml中配置。","link":"/2020/06/25/BigData/Hadoop/HDFS/"}],"tags":[{"name":"测试","slug":"测试","link":"/tags/%E6%B5%8B%E8%AF%95/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zookeeper","slug":"Zookeeper","link":"/tags/Zookeeper/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"}],"categories":[{"name":"分类测试","slug":"分类测试","link":"/categories/%E5%88%86%E7%B1%BB%E6%B5%8B%E8%AF%95/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zookeeper","slug":"大数据/Zookeeper","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/"},{"name":"Hadoop","slug":"大数据/Hadoop","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"}]}